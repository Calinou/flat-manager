#!/usr/bin/python3

# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.

import aiohttp
import asyncio
import gzip
import json
import logging
import os
import os
import sys

from aiohttp.abc import AbstractStreamWriter
from argparse import ArgumentParser
from multidict import CIMultiDict
from typing import (Any, Optional)
from functools import reduce

import gi
gi.require_version('OSTree', '1.0')
from gi.repository import GLib, Gio, OSTree

UPLOAD_CHUNK_LIMIT = 2 * 1024 * 1024
DEFAULT_LIMIT = 2 ** 16

# This is similar to the regular payload, but opens the file lazily
class NamedFilePayload(aiohttp.payload.Payload):
    def __init__(self,
                 value: str,
                 disposition: str='attachment',
                 *args: Any,
                 **kwargs: Any) -> None:
        self._file = None
        if 'filename' not in kwargs:
            kwargs['filename'] = os.path.basename(value)

        super().__init__(value, *args, **kwargs)

        if self._filename is not None and disposition is not None:
            self.set_content_disposition(disposition, filename=self._filename)

        self._size = os.stat(value).st_size

    async def write(self, writer: AbstractStreamWriter) -> None:
        if self._file is None:
            self._file = open(self._value, 'rb')
        try:
            chunk = self._file.read(DEFAULT_LIMIT)
            while chunk:
                await writer.write(chunk)
                chunk = self._file.read(DEFAULT_LIMIT)
        finally:
            self._file.close()

    @property
    def size(self) -> Optional[float]:
        return self._size


def ostree_object_path(repo, obj):
    repodir = repo.get_path().get_path()
    return os.path.join(repodir, 'objects', obj[0:2], obj[2:])

def ostree_get_dir_files(repo, objects, dirtree):
    if dirtree.endswith(".dirtree"):
        dirtree = dirtree[:-8]
    dirtreev = repo.load_variant(OSTree.ObjectType.DIR_TREE, dirtree)[1]
    iter = OSTree.RepoCommitTraverseIter()
    iter.init_dirtree(repo, dirtreev, 0)
    while True:
        type = iter.next()
        if type == OSTree.RepoCommitIterResult.END:
            break
        if type == OSTree.RepoCommitIterResult.ERROR:
            break
        if type == OSTree.RepoCommitIterResult.FILE:
            d = iter.get_file()
            objects.add(d.out_checksum + ".filez")
        if type == OSTree.RepoCommitIterResult.DIR:
            pass

def local_needed_files(repo, metadata_objects):
    objects = set()
    for c in metadata_objects:
        if c.endswith(".dirtree"):
            ostree_get_dir_files(repo, objects, c)
    return objects

def local_needed_metadata_dirtree(repo, objects, dirtree_content, dirtree_meta):
    objects.add(dirtree_meta + ".dirmeta")
    dirtree_content_name = dirtree_content + ".dirtree"
    if dirtree_content_name in objects:
        return
    objects.add(dirtree_content_name)

    dirtreev = repo.load_variant(OSTree.ObjectType.DIR_TREE, dirtree_content)[1]
    iter = OSTree.RepoCommitTraverseIter()
    iter.init_dirtree(repo, dirtreev, 0)
    while True:
        type = iter.next()
        if type == OSTree.RepoCommitIterResult.END:
            break
        if type == OSTree.RepoCommitIterResult.ERROR:
            break
        if type == OSTree.RepoCommitIterResult.FILE:
            pass
        if type == OSTree.RepoCommitIterResult.DIR:
            d = iter.get_dir()
            local_needed_metadata_dirtree(repo, objects, d.out_content_checksum, d.out_meta_checksum)

def local_needed_metadata(repo, commits):
    objects = set()
    for rev in commits:
        objects.add(rev + ".commit")
        commitv = repo.load_variant(OSTree.ObjectType.COMMIT, rev)[1]
        iter = OSTree.RepoCommitTraverseIter()
        iter.init_commit(repo, commitv, 0)
        while True:
            type = iter.next()
            if type == OSTree.RepoCommitIterResult.END:
                break
            if type == OSTree.RepoCommitIterResult.ERROR:
                break
            if type == OSTree.RepoCommitIterResult.FILE:
                pass
            if type == OSTree.RepoCommitIterResult.DIR:
                d = iter.get_dir()
                local_needed_metadata_dirtree(repo, objects, d.out_content_checksum, d.out_meta_checksum)
    return objects

async def missing_objects(session, remote, wanted):
    wanted_json=json.dumps({'wanted': wanted}).encode('utf-8')
    data=gzip.compress(wanted_json)
    headers = {
        'Content-Encoding': 'gzip',
        'Content-Type': 'application/json'
    }
    async with session.get(remote + "/missing_objects", data=data, headers=headers) as resp:
        assert 200 == resp.status ## TODO: Proper error handling
        data = await resp.json()
        return data;

async def upload_files(session, repo, files):
    if len(files) == 0:
        return
    print("Uploading %d objects (%d bytes)" % (len(files), reduce(lambda x, y: x + y, map(lambda f: f.size, files))))
    with aiohttp.MultipartWriter() as writer:
        for f in files:
            writer.append(f)
    resp = await session.request(
        "post", repo + '/upload',
        data=writer, headers=writer.headers)
    assert 200 == resp.status ## TODO: Proper error handling

async def upload_objects(session, repo, remote, objects):
    req = []
    total_size = 0
    for file_obj in objects:
        named = get_object_multipart(repo, file_obj)
        total_size = total_size + named.size
        req.append(named);

        if total_size > UPLOAD_CHUNK_LIMIT:
            await upload_files(session, remote, req)
            req = []
            total_size = 0

    await upload_files(session, remote, req)

def get_object_multipart(repo, object):
    return NamedFilePayload(repo + "/objects/" + object[:2] + "/" + object[2:], filename=object)

async def push_main():
    aparser = ArgumentParser(description='Push to repo manager')
    aparser.add_argument('--repo', help='local repository')
    aparser.add_argument('-v', '--verbose', action='store_true',
                         help='enable verbose output')
    aparser.add_argument('--debug', action='store_true',
                         help='enable debugging output')
    aparser.add_argument('remote', help='remote repository')
    aparser.add_argument('branch', nargs='*', help='branches to push')
    args = aparser.parse_args()

    loglevel = logging.WARNING
    if args.verbose:
        loglevel = logging.INFO
    if args.debug:
        loglevel = logging.DEBUG

    logging.basicConfig(format='%(module)s: %(levelname)s: %(message)s',
                        level=loglevel, stream=sys.stderr)

    if args.repo is None:
        print('No repo specified')
        exit(1)

    repo = OSTree.Repo.new(Gio.File.new_for_path(args.repo))
    repo.open(None)
    if len(args.branch) == 0:
        _, refs = repo.list_refs(None, None)
    else:
        refs = {}
        for branch in args.branch:
            _, rev = repo.resolve_rev(branch, False)
            refs[branch] = rev

    print("Uploading refs: %s"% (list(refs)))

    metadata_objects = local_needed_metadata(repo, refs.values())

    print("Refs contain %d metadata objects" % (len(metadata_objects)))

    async with aiohttp.ClientSession() as session:
        missing_metadata_objects = await missing_objects(session, args.remote, list(metadata_objects))

        print("Remote missing %d of those" % (len(missing_metadata_objects)))

        file_objects = local_needed_files(repo, missing_metadata_objects)
        print("Has %d file objects for those" % (len(file_objects)))

        missing_file_objects = await missing_objects(session, args.remote, list(file_objects))
        print("Remote missing %d of those" % (len(missing_file_objects)))

        # First upload all missing file objects
        print("Uploading file objects")
        await upload_objects(session, args.repo, args.remote, missing_file_objects)

        # Then all the metadata
        print("Uploading metadata objects")
        await upload_objects(session, args.repo, args.remote, missing_file_objects)

        # Then the refs
        # TODO

if __name__ == '__main__':
    progname = os.path.basename(sys.argv[0])

    loop = asyncio.get_event_loop()
    loop.run_until_complete(push_main())
